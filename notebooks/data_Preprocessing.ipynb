{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dKimlvPOODJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess text data, handling NaN values\n",
        "def preprocess_text(text):\n",
        "    # Check if the text is NaN (float) or not a string\n",
        "    if pd.isna(text):\n",
        "        text = \"\"  # Replace NaN or non-string values with an empty string\n",
        "    elif not isinstance(text, str):\n",
        "        text = str(text)  # Ensure it's a string\n",
        "\n",
        "    # Remove special characters and punctuation\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "\n",
        "    print(f\"Preprocessing text: {text[:50]}...\")  # Show first 50 characters for preview\n",
        "    return text"
      ],
      "metadata": {
        "id": "ZYZXo8S1OWHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to label a subset of the dataset in CoNLL format\n",
        "def label_dataset(data, output_file):\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for index, row in data.iterrows():\n",
        "            text = row['preprocessed_text']\n",
        "            # Tokenize the text and label entities manually or using a pre-trained NER model\n",
        "            # Here we simply label them as O (outside any entity) for demonstration purposes\n",
        "            for token in text.split():\n",
        "                f.write(f\"{token}\\tO\\n\")\n",
        "            f.write(\"\\n\")\n",
        "        print(f\"Saved labeled data in CoNLL format to {output_file}.\")"
      ],
      "metadata": {
        "id": "YecKyoxvOX90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load all individual channel CSV files and concatenate them into one DataFrame\n",
        "def load_and_concatenate_csv_files(output_file, channels_folder='.', file_extension='.csv'):\n",
        "    all_data = []\n",
        "\n",
        "    # Iterate over all files in the folder\n",
        "    for filename in os.listdir(channels_folder):\n",
        "        if filename.endswith(file_extension):\n",
        "            file_path = os.path.join(channels_folder, filename)\n",
        "            df = pd.read_csv(file_path)\n",
        "            all_data.append(df)\n",
        "\n",
        "    # Concatenate all dataframes\n",
        "    if all_data:\n",
        "        combined_df = pd.concat(all_data, ignore_index=True)\n",
        "        combined_df.to_csv(output_file, index=False)\n",
        "        print(f\"All data saved to {output_file}.\")\n",
        "    else:\n",
        "        print(\"No CSV files found to concatenate.\")"
      ],
      "metadata": {
        "id": "1S6ZwCdqOaI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load, preprocess, and label data\n",
        "def process_data():\n",
        "    # Concatenate individual CSV files into 'telegram_data.csv'\n",
        "    load_and_concatenate_csv_files('telegram_data.csv')\n",
        "\n",
        "    # Load the concatenated data\n",
        "    data = pd.read_csv('telegram_data.csv')\n",
        "    print(f\"Loaded {len(data)} rows of data from telegram_data.csv\")\n",
        "\n",
        "    # Preprocess the text data\n",
        "    data['preprocessed_text'] = data['text'].apply(preprocess_text)\n",
        "    print(\"Text data preprocessing completed.\\n\")\n",
        "\n",
        "    # Label a subset of the data\n",
        "    label_dataset(data.head(50), 'labeled_telegram_data_conll.txt')\n",
        "    print(\"Labeling of data completed.\\n\")\n",
        "\n",
        "# Run the process\n",
        "process_data()"
      ],
      "metadata": {
        "id": "Bi0nduGcOd4R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}