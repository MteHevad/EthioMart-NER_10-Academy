{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dKimlvPOODJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from seqeval.metrics import classification_report\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the correct path to your labeled dataset in CoNLL format\n",
        "filepath = 'C:/Users/hp/Desktop/KAIM/Week 5/labeled_telegram_data_conll.txt'"
      ],
      "metadata": {
        "id": "ZYZXo8S1OWHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the file exists before proceeding\n",
        "if not os.path.exists(filepath):\n",
        "    raise FileNotFoundError(f\"The file was not found at the specified path: {filepath}\")"
      ],
      "metadata": {
        "id": "YecKyoxvOX90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load the dataset in CoNLL format\n",
        "def load_conll_format(filepath):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        sentence = []\n",
        "        label = []\n",
        "        for line in f:\n",
        "            if line == \"\\n\":  # Sentence boundary\n",
        "                if sentence:  # Check if sentence is not empty\n",
        "                    sentences.append(sentence)\n",
        "                    labels.append(label)\n",
        "                sentence = []\n",
        "                label = []\n",
        "            else:\n",
        "                token, tag = line.strip().split()\n",
        "                sentence.append(token)\n",
        "                label.append(tag)\n",
        "    return sentences, labels"
      ],
      "metadata": {
        "id": "1S6ZwCdqOaI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CoNLL dataset\n",
        "sentences, labels = load_conll_format(filepath)"
      ],
      "metadata": {
        "id": "Bi0nduGcOd4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to a pandas DataFrame and then to a Hugging Face Dataset\n",
        "df = pd.DataFrame({\"tokens\": sentences, \"ner_tags\": labels})\n",
        "dataset = Dataset.from_pandas(df)"
      ],
      "metadata": {
        "id": "Syd84VLqc75j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained tokenizer and model\n",
        "model_checkpoint = \"xlm-roberta-base\"  # You can switch to \"bert-tiny-amharic\" if available\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "ZdtHdrjCc-vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define label mapping\n",
        "label_list = ['O', 'B-Product', 'I-Product', 'B-LOC', 'I-LOC', 'B-PRICE', 'I-PRICE']\n",
        "label_to_id = {label: i for i, label in enumerate(label_list)}"
      ],
      "metadata": {
        "id": "ZjKrhPDYdA_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to tokenize the dataset and align labels with tokens\n",
        "def tokenize_and_align_labels(batch):\n",
        "    tokenized_inputs = tokenizer(batch['tokens'], truncation=True, is_split_into_words=True, padding=True)\n",
        "    labels = []\n",
        "    for i, label in enumerate(batch['ner_tags']):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        aligned_labels = []\n",
        "        previous_word_idx = None\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                aligned_labels.append(-100)  # Ignore subwords and special tokens\n",
        "            elif word_idx != previous_word_idx:  # Beginning of a new word\n",
        "                aligned_labels.append(label_to_id[label[word_idx]])\n",
        "            else:\n",
        "                aligned_labels.append(label_to_id[label[word_idx]])  # Continuation of the same word\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(aligned_labels)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "FrTm3fgmdDKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the dataset and align the labels\n",
        "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "id": "twiH0XUWdFeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set training arguments with matching save_strategy and evaluation_strategy\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./ner_model\",\n",
        "    evaluation_strategy=\"epoch\",  # Evaluating at the end of each epoch\n",
        "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True  # Load the best model based on evaluation metric at the end\n",
        ")"
      ],
      "metadata": {
        "id": "DAUuD7CUdHyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model for token classification\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
      ],
      "metadata": {
        "id": "z4qnPdQ7dKZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator to pad sequences for token classification\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ],
      "metadata": {
        "id": "iCPpGLmodM-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into training and validation sets\n",
        "train_test_split = tokenized_datasets.train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "eval_dataset = train_test_split[\"test\"]"
      ],
      "metadata": {
        "id": "Ro4kmqb6dPda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Hugging Face Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=lambda p: classification_report(p.label_ids, p.predictions.argmax(-1), output_dict=True)\n",
        ")"
      ],
      "metadata": {
        "id": "jgFn6iGwdRXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "YnwWs4d8dT8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model performance on the validation set\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "Z2yUtG7JdV7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model for future use\n",
        "trainer.save_model(\"./fine_tuned_ner_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_ner_model\")"
      ],
      "metadata": {
        "id": "dvVuvSD1dXyj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}