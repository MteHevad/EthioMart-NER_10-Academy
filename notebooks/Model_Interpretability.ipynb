{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrixTJvtr8nP"
      },
      "outputs": [],
      "source": [
        "!pip install shap lime transformers torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import torch\n",
        "from transformers import XLMRobertaForTokenClassification, DistilBertForTokenClassification\n",
        "from transformers import XLMRobertaTokenizer, DistilBertTokenizer\n",
        "import shap\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "-d0v9C6kwj3t"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load models and tokenizers\n",
        "model_name_xlm = 'xlm-roberta-base'  # XLM-Roberta\n",
        "model_name_distil = 'distilbert-base-multilingual-cased'  # DistilBERT\n",
        "\n",
        "tokenizer_xlm = XLMRobertaTokenizer.from_pretrained(model_name_xlm)\n",
        "tokenizer_distil = DistilBertTokenizer.from_pretrained(model_name_distil)"
      ],
      "metadata": {
        "id": "0_14U0UVwojd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load models\n",
        "model_xlm = XLMRobertaForTokenClassification.from_pretrained(model_name_xlm)\n",
        "model_distil = DistilBertForTokenClassification.from_pretrained(model_name_distil)"
      ],
      "metadata": {
        "id": "nh1nWf1hwtqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set models to evaluation mode\n",
        "model_xlm.eval()\n",
        "model_distil.eval()"
      ],
      "metadata": {
        "id": "TzMLV06CwxSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample tokenized input data\n",
        "sample_data = [\n",
        "    \"John Smith visited New York City.\",\n",
        "    \"The river bank near the bank of America was flooded.\"\n",
        "]"
      ],
      "metadata": {
        "id": "Bv6kmzi5w06t"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to tokenize input for SHAP\n",
        "def tokenize_for_shap(text):\n",
        "    return tokenizer_xlm(text, return_tensors=\"pt\", padding=True, truncation=True)"
      ],
      "metadata": {
        "id": "AQOrLz_5w3rt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SHAP Implementation\n",
        "def explain_with_shap(model, text):\n",
        "    # Tokenize input\n",
        "    tokenized_data = tokenize_for_shap(text)\n",
        "\n",
        "    # Define a function to make predictions\n",
        "    def predict_fn(input_ids):\n",
        "        input_ids = torch.tensor(input_ids)  # Ensure input_ids is a tensor\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits\n",
        "            return torch.softmax(logits, dim=2).cpu().numpy()  # Get probabilities\n",
        "\n",
        "    # Create explainer\n",
        "    explainer = shap.KernelExplainer(predict_fn, tokenized_data['input_ids'].cpu().numpy())\n",
        "\n",
        "    # Compute SHAP values for the tokenized input\n",
        "    shap_values = explainer.shap_values(tokenized_data['input_ids'].cpu().numpy())\n",
        "\n",
        "    # Plot SHAP summary for the first class (change index if necessary)\n",
        "    feature_names = tokenizer_xlm.convert_ids_to_tokens(tokenized_data['input_ids'][0].tolist())\n",
        "    shap.summary_plot(shap_values[0], feature_names=feature_names)"
      ],
      "metadata": {
        "id": "VyIRnXn6w7Q9"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explain_with_lime(model, text):\n",
        "    explainer = LimeTextExplainer(class_names=[\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\"])  # Adjust class names as necessary\n",
        "\n",
        "    # Define a prediction function for LIME\n",
        "    def predict_fn(texts):\n",
        "        inputs = tokenizer_distil(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs['input_ids'].to(model.device))\n",
        "            logits = outputs.logits\n",
        "            probabilities = torch.softmax(logits, dim=2)  # Shape (batch_size, seq_length, num_classes)\n",
        "            return probabilities.cpu().numpy()  # Return as numpy array\n",
        "\n",
        "    # Generate LIME explanation\n",
        "    explanation = explainer.explain_instance(text, predict_fn, num_features=10)\n",
        "\n",
        "    # Store results for reporting\n",
        "    lime_analysis = {\n",
        "        'lime_exp': explanation,\n",
        "        'text': text\n",
        "    }\n",
        "\n",
        "    return lime_analysis"
      ],
      "metadata": {
        "id": "hCJmOZStxA4b"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explain the models\n",
        "print(\"SHAP Explanation for XLM-Roberta:\")\n",
        "explain_with_shap(model_xlm, sample_data[0])\n",
        "print(\"LIME Explanation for DistilBERT:\")\n",
        "explain_with_lime(model_distil, sample_data[1])"
      ],
      "metadata": {
        "id": "Wkoei9rJxD5f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}